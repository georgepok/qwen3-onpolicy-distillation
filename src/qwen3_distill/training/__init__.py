"""
Training loop using unsloth for memory-efficient fine-tuning.

This module implements per-token advantage computation and
gradient accumulation for reverse KL divergence optimization.
"""

# from .trainer import DistillationTrainer
# from .config import TrainingConfig
# from .loss import ReverseKLLoss

__all__ = [
    # "DistillationTrainer",
    # "TrainingConfig",
    # "ReverseKLLoss",
]
